name: "LSTM"

data:
  return_type: "fna" # "fna" "hmm_match_sequence"
  pad_pack: True # Necessary when working with non-fixed length seqs.
  use_saved: False # Use pickled input if available.

params:
  _target_: src.models.LSTM_collection.BasicLSTM
  dim_shift: True
  num_classes: 2
  fc_dropout_rate: 0.20
  activation_fn: "ReLU"
  fc_num: 1 # number of fully-connected layers after CNN/LSTM

  input_size: 5 # Sequence feature length. Set automatically if embeddinglayer.
  hidden_size_lstm: 1
  num_layers_lstm: 1
  embedding_dim: 0 # Set to 0 if not using embedding layer.

  pad_pack: True # Necessary when working with non-fixed length seqs.
  vocab_size: ${vocab_size} # Auto-populated by trainer.
