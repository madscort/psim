project: "psim"
accelerator: "gpu"
devices: -1
dataset: "phage_25_fixed_25000_reduced_90_ws"
batch_size: 16
num_workers: 8
epochs: 30

data:
  # LSTM only
  return_type: "hmm_match_sequence" # "fna"
  pad_pack: True # Necessary when working with non-fixed length seqs.
  use_saved: True # Use pickled input if available.

model:
  type: "Transformer"
  name: "BasicTransformer"
  fc_dropout_rate: 0.5
  batchnorm: False
  activation_fn: "ReLU"
  fc_num: 1 # number of fully-connected layers after CNN/LSTM
  
  # Basic CNN only
  alt_dropout_rate: 0.0 # dropout on CNNs
  kernel_size_1: 5
  kernel_size_2: 7
  kernel_size_3: 0 # If zero - no third layer.

  # Inception only
  num_inception_layers: 1
  out_channels: 32
  kernel_size_b1: 3
  kernel_size_b2: 3
  keep_b3: False
  keep_b4: True

  # LSTM only
  input_size: 1 # Sequence feature length. Set automatically if embeddinglayer.
  hidden_size_lstm: 2
  num_layers_lstm: 1
  embedding_dim: 5 # Set to None if not using embedding layer.

optimizer:
  name: "adamw"
  lr: 0.007
