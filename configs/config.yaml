project: "psim"
accelerator: "gpu"
devices: -1
dataset: "prophage_95_fixed_25000_ps_reduced_90_ws"
batch_size: 32
num_workers: 8
epochs: 10

data:
  # LSTM only
  return_type: "fna" # "fna" "hmm_match_sequence"
  pad_pack: False # Necessary when working with non-fixed length seqs.
  use_saved: False # Use pickled input if available.

model:
  type: "CNNInception"
  name: "BasicInception"
  fc_dropout_rate: 0.20
  batchnorm: True
  activation_fn: "ELU"
  fc_num: 1 # number of fully-connected layers after CNN/LSTM
  
  # Basic CNN only
  alt_dropout_rate: 0.0 # dropout on CNNs
  kernel_size_1: 5
  kernel_size_2: 7
  kernel_size_3: 0 # If zero - no third layer.

  # Inception only
  num_inception_layers: 2
  out_channels: 16
  kernel_size_b1: 5
  kernel_size_b2: 121
  keep_b3: False
  keep_b4: False

  # LSTM only
  input_size: 1 # Sequence feature length. Set automatically if embeddinglayer.
  hidden_size_lstm: 2
  num_layers_lstm: 1
  embedding_dim: 5 # Set to None if not using embedding layer.

optimizer:
  name: "adamw"
  lr: 0.005
